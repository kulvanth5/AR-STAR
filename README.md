In this work, we enhance the performance of a multi-stage convolutional RNN by modifying the STAR (Self-gated Recurrent Attention) cell’s activation function. Traditionally, the STAR cell uses the tanh activation, which, while common, has limitations due to its symmetrical nature around zero and its tendency to saturate. To address these, we replaced tanh with an Attention-based ReLU activation, which selectively amplifies positive features and suppresses negative ones. This modification aims to improve feature separation, making relevant features more prominent for easier model differentiation.

![image](https://github.com/user-attachments/assets/7666923c-ace0-4678-8e24-5c387fcd6acd)

The Attention-based ReLU effectively highlights critical aspects of feature distinction within the data. By amplifying positive features, it accentuates components associated with desired outputs, making them more accessible to the network’s decision process. Meanwhile, suppressing negative features reduces noise, preventing weak or irrelevant signals from influencing the model. This change enables the STAR cell to better capture temporal patterns and improves model focus on essential information. Overall, it supports the network’s resilience against the gradient vanishing issue that affects deep RNNs.

Experimentally, this adjustment resulted in a noticeable boost in model performance, as shown by a 4% increase in the F1 score, from 52% to 56%. This improvement reflects a stronger balance of precision and recall, crucial for effectively handling complex data. Such results confirm the effectiveness of Attention-based ReLU in enhancing feature discrimination within the model. By refining the STAR cell's activation, we demonstrate that targeted modifications can significantly advance performance in RNN-based architectures.
